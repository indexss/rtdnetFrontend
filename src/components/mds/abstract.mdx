# Abstract

Accurate gaze prediction is crucial in various fields. Recently, predicting gaze direction from videos has gained increasing attention, but traditional methods mainly rely on static images, making it challenging to leverage temporal and motion information. To address this, we proposed RTDNet, a Recurrent Network using Temporal Difference for video gaze estimation, which integrates inter-frame motion and temporal sequence information to improve accuracy. First, we introduce the Temporal Difference Module (TDModule) to capture inter-frame motion information for better gaze estimation. It extracts motion information through temporal differencing and grouped convolution. We then upsample these features to create a more complete motion representation, combining them with original video features at different levels through a recurrent structure. We also employ Transformer to enhance the processing of temporal sequences. Additionally, we introduce Point Distribution Alignment Loss (PDA Loss), which aligns Points of Gaze (PoG) trajectories by using sequence information, further improving accuracy. Our approach achieves state-of-the-art performance on the EVE and EYEDIAP datasets.